{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2734/miniconda3/envs/worldbank/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "#gets all this setup\n",
    "import time\n",
    "start_time = time.time()\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "# from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc, accuracy\n",
    "from fast_bert.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "\n",
    "\n",
    "def create_model(columnm, epoch):\n",
    "    \n",
    "#     if not os.path.exists('/scratch/da2734/twitter/mturk_mar6/log_{}/'.format(column)):\n",
    "#         os.makedirs('/scratch/da2734/twitter/mturk_mar6/log_{}/'.format(column))\n",
    "\n",
    "#     if not os.path.exists('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column)):\n",
    "#         os.makedirs('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column))\n",
    "\n",
    "    LOG_PATH = Path('/scratch/da2734/twitter/mturk_mar6/log_{}/'.format(column))\n",
    "    DATA_PATH = Path('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/')\n",
    "    LABEL_PATH = Path('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/')\n",
    "    OUTPUT_PATH = Path('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_{}'.format(column))\n",
    "    FINETUNED_PATH = None\n",
    "\n",
    "    args = Box({\n",
    "        \"run_text\": \"labor mturk ar 6 binary\",\n",
    "        \"train_size\": -1,\n",
    "        \"val_size\": -1,\n",
    "        \"log_path\": LOG_PATH,\n",
    "        \"full_data_dir\": DATA_PATH,\n",
    "        \"data_dir\": DATA_PATH,\n",
    "        \"task_name\": \"labor_market_classification\",\n",
    "        \"no_cuda\": False,\n",
    "        #     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "        \"output_dir\": OUTPUT_PATH,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"do_lower_case\": True,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_train_epochs\": 100,\n",
    "        \"warmup_proportion\": 0.0,\n",
    "        \"no_cuda\": False,\n",
    "        \"local_rank\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"optimize_on_cpu\": False,\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"max_steps\": -1,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"logging_steps\": 50,\n",
    "        \"eval_all_checkpoints\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"overwrite_cache\": True,\n",
    "        \"seed\": 42,\n",
    "        \"loss_scale\": 128,\n",
    "        \"task_name\": 'intent',\n",
    "        \"model_name\": 'bert-base-uncased',\n",
    "        \"model_type\": 'bert'\n",
    "    })\n",
    "\n",
    "    import logging\n",
    "\n",
    "    logfile = str(LOG_PATH / 'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "        handlers=[\n",
    "            logging.FileHandler(logfile),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ])\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    logger.info(args)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        args.multi_gpu = True\n",
    "    else:\n",
    "        args.multi_gpu = False\n",
    "\n",
    "    label_cols = ['pos']\n",
    "\n",
    "    databunch = BertDataBunch(\n",
    "        args['data_dir'],\n",
    "        LABEL_PATH,\n",
    "        args.model_name,\n",
    "        train_file='train_{}.csv'.format(column),\n",
    "        val_file='val_{}.csv'.format(column),\n",
    "        label_file='label_{}.csv'.format(column),\n",
    "        # test_data='test.csv',\n",
    "        text_col=\"text\",  # this is the name of the column in the train file that containts the tweet text\n",
    "        label_col=label_cols,\n",
    "        batch_size_per_gpu=args['train_batch_size'],\n",
    "        max_seq_length=args['max_seq_length'],\n",
    "        multi_gpu=args.multi_gpu,\n",
    "        multi_label=False,\n",
    "        model_type=args.model_type)\n",
    "\n",
    "    num_labels = len(databunch.labels)\n",
    "    print('num_labels', num_labels)\n",
    "\n",
    "    print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "    # metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "    metrics = []\n",
    "    # metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "    # metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "    # metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "    metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "    metrics.append({'name': 'roc_auc_save_to_plot_binary', 'function': roc_auc_save_to_plot_binary})\n",
    "    # metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n",
    "\n",
    "    learner = BertLearner.from_pretrained_model(\n",
    "        databunch,\n",
    "        pretrained_path='../mturk_mar6/output_binary_pos_neg_balanced_{}/model_out_{}/'.format(column, epoch), \n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "        logger=logger,\n",
    "        output_dir=args.output_dir,\n",
    "        finetuned_wgts_path=FINETUNED_PATH,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        multi_gpu=args.multi_gpu,\n",
    "        is_fp16=args.fp16,\n",
    "        multi_label=False,\n",
    "        logging_steps=0)\n",
    "    \n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_unemployed\n",
      "04/11/2020 16:38:06 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_is_unemployed'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_is_unemployed'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 100, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "04/11/2020 16:38:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/11/2020 16:38:07 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_train_multi_class_512_train_is_unemployed.csv\n",
      "04/11/2020 16:38:07 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_dev_multi_class_512_val_is_unemployed.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 76.01465845108032 seconds\n",
      "04/11/2020 16:38:07 - INFO - transformers.configuration_utils -   loading configuration file ../mturk_mar6/output_binary_pos_neg_balanced_is_unemployed/model_out_10/config.json\n",
      "04/11/2020 16:38:07 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/11/2020 16:38:07 - INFO - transformers.modeling_utils -   loading weights file ../mturk_mar6/output_binary_pos_neg_balanced_is_unemployed/model_out_10/pytorch_model.bin\n",
      "load model: 87.43490529060364 seconds\n",
      "(349, 4)\n",
      "04/11/2020 16:38:19 - INFO - root -   Writing example 0 of 349\n",
      "inference: 92.54621624946594 seconds on  349 tweets\n",
      "     id  pos_model  neg_model  \\\n",
      "0  5350  0.800060   0.199940    \n",
      "1  2889  0.918375   0.081625    \n",
      "2  3796  0.992124   0.007876    \n",
      "3  641   0.974355   0.025645    \n",
      "4  1414  0.989602   0.010398    \n",
      "\n",
      "                                                                                                                                               text  \\\n",
      "0  I recently came down off my high horse. Eh it was a self appointed position anyway. #1reflection                                                   \n",
      "1  Got to work at 6am &amp; not long after I got scolded by a co-worker over something small that I missed doing a different position yesterday.      \n",
      "2  #Accounting #Job alert: Controller | Robert Half Management Resources | #CLEVELAND  OH http://t.co/LrNHsZkkjr #Jobs                                \n",
      "3  Just got hired to clean another store at water side shops named Panerai boutique .. Does anyone know what kind store is that ?                     \n",
      "4  Unemployed Under-Employed Unhappily Employed? Find jobs &amp; career solutions on LinkedIn in JCD The Job Seeker &amp;... http://t.co/x7VIlwY9nC   \n",
      "\n",
      "   pos  neg  \n",
      "0  0    1    \n",
      "1  0    1    \n",
      "2  0    1    \n",
      "3  0    1    \n",
      "4  0    1    \n",
      "lost_job_1mo\n",
      "04/11/2020 16:38:24 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_lost_job_1mo'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_lost_job_1mo'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 100, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "04/11/2020 16:38:24 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/11/2020 16:38:24 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_train_multi_class_512_train_lost_job_1mo.csv\n",
      "04/11/2020 16:38:24 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_dev_multi_class_512_val_lost_job_1mo.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 92.79438781738281 seconds\n",
      "04/11/2020 16:38:24 - INFO - transformers.configuration_utils -   loading configuration file ../mturk_mar6/output_binary_pos_neg_balanced_lost_job_1mo/model_out_14/config.json\n",
      "04/11/2020 16:38:24 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/11/2020 16:38:24 - INFO - transformers.modeling_utils -   loading weights file ../mturk_mar6/output_binary_pos_neg_balanced_lost_job_1mo/model_out_14/pytorch_model.bin\n",
      "load model: 95.71172738075256 seconds\n",
      "(176, 4)\n",
      "04/11/2020 16:38:27 - INFO - root -   Writing example 0 of 176\n",
      "inference: 97.72191667556763 seconds on  176 tweets\n",
      "     id  pos_model  neg_model  \\\n",
      "0  2357  0.992483   0.007517    \n",
      "1  3506  0.361681   0.638319    \n",
      "2  3368  0.990608   0.009392    \n",
      "3  4980  0.877018   0.122982    \n",
      "4  1110  0.992669   0.007331    \n",
      "\n",
      "                                                                                                                                     text  \\\n",
      "0  Does anyone know any places hiring? #needajob                                                                                            \n",
      "1  Thomas Pridgen was once asked what the secret to his success was and he said I never worked a day job.And that is why I am unemployed.   \n",
      "2  Social Media Friend or Foe? Some Use Social Media to Get Hired Others to Get Fired http://t.co/AswAEWYvfw #socialmedia                   \n",
      "3  Due to the politics in my office I am unable to work on certain days therefore my asking to switch days this week was denied.            \n",
      "4  I am job searching for: Work at home jobs! http://t.co/4tgcXWg6JK — Walker Inklebarger (@Walker_marocliv.... http://t.co/Wzgp5pE2qL      \n",
      "\n",
      "   pos  neg  \n",
      "0  0    1    \n",
      "1  0    1    \n",
      "2  0    1    \n",
      "3  0    1    \n",
      "4  0    1    \n",
      "job_search\n",
      "04/11/2020 16:38:29 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_job_search'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_job_search'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 100, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/11/2020 16:38:29 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/11/2020 16:38:29 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_train_multi_class_512_train_job_search.csv\n",
      "04/11/2020 16:38:29 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_dev_multi_class_512_val_job_search.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 97.97850847244263 seconds\n",
      "04/11/2020 16:38:29 - INFO - transformers.configuration_utils -   loading configuration file ../mturk_mar6/output_binary_pos_neg_balanced_job_search/model_out_7/config.json\n",
      "04/11/2020 16:38:29 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/11/2020 16:38:29 - INFO - transformers.modeling_utils -   loading weights file ../mturk_mar6/output_binary_pos_neg_balanced_job_search/model_out_7/pytorch_model.bin\n",
      "load model: 100.70926856994629 seconds\n",
      "(236, 4)\n",
      "04/11/2020 16:38:32 - INFO - root -   Writing example 0 of 236\n",
      "inference: 103.46410036087036 seconds on  236 tweets\n",
      "     id  neg_model  pos_model  \\\n",
      "0  4572  0.782649   0.217351    \n",
      "1  5273  0.781155   0.218845    \n",
      "2  4374  0.143632   0.856368    \n",
      "3  3239  0.271822   0.728178    \n",
      "4  3719  0.610427   0.389573    \n",
      "\n",
      "                                                                                                                                                                                                                       text  \\\n",
      "0  Looking for work?  Are you unemployed?  We are now hiring. http://t.co/uwCbHWfUuh http://t.co/5XtEXTT8e8                                                                                                                   \n",
      "1  Dont submit then quit. Apply for jobs like Java developer at Cognizant then look for people you might know at the company. You might be able to reach out to HR directly. Ready to apply? Check out the link in our bio.   \n",
      "2  #VPDebate shots fired shots fired lol get em Tim #ImWithHer                                                                                                                                                                \n",
      "3  Got fired on Wednesday got my job back yesterday and I work today haYesterday was my day off too so its like I was never fired at allMy manager hates me even more now I know it aha                                       \n",
      "4  Now my employer has Minimum Wage as their ringtone. This pleases me more than is probably reasonable.                                                                                                                      \n",
      "\n",
      "   pos  neg  \n",
      "0  0    1    \n",
      "1  0    1    \n",
      "2  0    1    \n",
      "3  0    1    \n",
      "4  0    1    \n",
      "is_hired_1mo\n",
      "04/11/2020 16:38:35 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_is_hired_1mo'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_is_hired_1mo'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 100, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "04/11/2020 16:38:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/11/2020 16:38:35 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_train_multi_class_512_train_is_hired_1mo.csv\n",
      "04/11/2020 16:38:35 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_dev_multi_class_512_val_is_hired_1mo.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 103.7147765159607 seconds\n",
      "04/11/2020 16:38:35 - INFO - transformers.configuration_utils -   loading configuration file ../mturk_mar6/output_binary_pos_neg_balanced_is_hired_1mo/model_out_12/config.json\n",
      "04/11/2020 16:38:35 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/11/2020 16:38:35 - INFO - transformers.modeling_utils -   loading weights file ../mturk_mar6/output_binary_pos_neg_balanced_is_hired_1mo/model_out_12/pytorch_model.bin\n",
      "load model: 106.40172553062439 seconds\n",
      "(170, 4)\n",
      "04/11/2020 16:38:37 - INFO - root -   Writing example 0 of 170\n",
      "inference: 108.32422256469727 seconds on  170 tweets\n",
      "     id  pos_model  neg_model  \\\n",
      "0  3992  0.897018   0.102982    \n",
      "1  3146  0.940871   0.059129    \n",
      "2  4577  0.950267   0.049733    \n",
      "3  2108  0.943895   0.056105    \n",
      "4  356   0.941710   0.058291    \n",
      "\n",
      "                                                                                                                                         text  \\\n",
      "0  If your unemployed or underemployed. This is the Career for you.                                                                             \n",
      "1  Have you been laid off fired or just downsized and need financial planning help? http://t.co/Gx9E39HENl                                      \n",
      "2  Apply Today: Are you searching for a Social Media Job? Use The Social Web App. https://t.co/p2cUeub9bH #unemployed #jobs                     \n",
      "3  Searching for #work in New Brunswick NJ? Send your #resume to Rutgers The State University of New Jersey. http://t.co/bzFgC2m2V8 #gethired   \n",
      "4  I am job searching for: Work at home jobs! http://t.co/4tgcXWg6JK — Walker Inklebarger (@Walker_marocliv.... http://t.co/wResUHuArJ          \n",
      "\n",
      "   pos  neg  \n",
      "0  0    1    \n",
      "1  0    1    \n",
      "2  0    1    \n",
      "3  0    1    \n",
      "4  0    1    \n",
      "job_offer\n",
      "04/11/2020 16:38:39 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_job_offer'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_job_offer'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 100, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/11/2020 16:38:39 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "04/11/2020 16:38:40 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_train_multi_class_512_train_job_offer.csv\n",
      "04/11/2020 16:38:40 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/cache/cached_bert_dev_multi_class_512_val_job_offer.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 108.63996362686157 seconds\n",
      "04/11/2020 16:38:40 - INFO - transformers.configuration_utils -   loading configuration file ../mturk_mar6/output_binary_pos_neg_balanced_job_offer/model_out_7/config.json\n",
      "04/11/2020 16:38:40 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/11/2020 16:38:40 - INFO - transformers.modeling_utils -   loading weights file ../mturk_mar6/output_binary_pos_neg_balanced_job_offer/model_out_7/pytorch_model.bin\n",
      "load model: 111.34006309509277 seconds\n",
      "(404, 4)\n",
      "04/11/2020 16:38:42 - INFO - root -   Writing example 0 of 404\n",
      "inference: 115.8809244632721 seconds on  404 tweets\n",
      "     id  neg_model  pos_model  \\\n",
      "0  4641  0.947527   0.052473    \n",
      "1  1549  0.008018   0.991982    \n",
      "2  365   0.050164   0.949836    \n",
      "3  3969  0.005283   0.994717    \n",
      "4  3670  0.913209   0.086791    \n",
      "\n",
      "                                                                                                                                        text  \\\n",
      "0  has anyone worked part time cont ed or volunteered during their first year? etc etc #DPTstudent                                             \n",
      "1  2nd week on the job and I may have already gotten someone fired I mean freed up ones future. Being fired is an opportunity. Your welcome.   \n",
      "2  I am trying to find a new avenue for work. I do not want a disability!                                                                      \n",
      "3  i got hired at that cleaning company but now my boss is in jail????                                                                         \n",
      "4  Looking to save time and #money? With InfoChip the inspection recertification or other action can be completed on a handheld device.        \n",
      "\n",
      "   pos  neg  \n",
      "0  0    1    \n",
      "1  0    1    \n",
      "2  0    1    \n",
      "3  0    1    \n",
      "4  0    1    \n"
     ]
    }
   ],
   "source": [
    "# histograms compute\n",
    "import pickle\n",
    "best_epochs = {\n",
    "    'is_hired_1mo':12,\n",
    "    'lost_job_1mo':14,\n",
    "    'job_offer':7,\n",
    "    'is_unemployed':10,\n",
    "    'job_search':7\n",
    "}\n",
    "\n",
    "all_pred = {}\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print(column)\n",
    "    start = time.time()\n",
    "    trained_model = create_model(column, best_epochs[column])\n",
    "    print('load model:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "    start = time.time()\n",
    "    val = pd.read_csv('/scratch/da2734/twitter/mturk_mar6/data_binary_pos_neg_balanced/val_{}.csv'.format(column))\n",
    "    print(val.shape)\n",
    "    # start_time = time.time()\n",
    "    texts = val['text'].values.tolist()\n",
    "#     print(len(texts))\n",
    "    predictions = trained_model.predict_batch(texts)\n",
    "#     print(predictions)\n",
    "    print('inference:', str(time.time() - start_time), 'seconds on ', len(texts), 'tweets')\n",
    "\n",
    "    prediction_df = pd.DataFrame(\n",
    "    [dict(prediction) for prediction in predictions],\n",
    "        index=val.id\n",
    "        ).rename(columns={\n",
    "        'pos':'pos_model',\n",
    "        'neg':'neg_model',\n",
    "    #     'is_hired_1mo':'is_hired_1mo_model',\n",
    "    #     'lost_job_1mo':'lost_job_1mo_model',\n",
    "    #     'job_offer\"':'job_offer_model',\n",
    "        }\n",
    "    )\n",
    "        \n",
    "    merged = prediction_df.merge(val, on='id')\n",
    "    print(merged.head())\n",
    "    \n",
    "    all_pred[column] = merged\n",
    "#     break\n",
    "\n",
    "pickle.dump( all_pred, open( \"./all_pred_8.2.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# histograms plot\n",
    "\n",
    "import pickle\n",
    "\n",
    "# with open(\"./all_pred_8.2.pkl\", 'rb') as f:\n",
    "#     # The protocol version used is detected automatically, so we do not\n",
    "#     # have to specify it.\n",
    "#     all_pred = pickle.load(f)\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print(column)\n",
    "    merged = all_pred[column]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    plt.hist(merged['pos'], bins=20, density = True, color = 'green', alpha = 0.2)\n",
    "    plt.hist(merged['neg'], bins=20, density = True, color = 'green', alpha = 0.2)\n",
    "\n",
    "    plt.hist(merged['pos_model'], bins=20, density = True, color = 'red', alpha = 0.2)\n",
    "#     plt.hist(merged['neg'], bins=20, density = True, color = 'green', alpha = 0.2)\n",
    "    plt.hist(merged['neg_model'], bins=20, density = True, color = 'blue', alpha = 0.2)\n",
    "#     ax.set_yscale('log')\n",
    "\n",
    "    # plt..yscale(value)\n",
    "    plt.title(column)\n",
    "#     ax.legend()\n",
    "    plt.xlim(0,1.1)\n",
    "    \n",
    "    merged.to_csv('merged_{}.csv'.format(column))\n",
    "\n",
    "#     plt.savefig('/scratch/da2734/twitter/mturk_mar6/data_binary/plot_{}.png'.format(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing ROC using learner.validate()\n",
    "ROC_dict = {}\n",
    "\n",
    "best_epochs = {\n",
    "    'is_hired_1mo':12,\n",
    "    'lost_job_1mo':14,\n",
    "    'job_offer':7,\n",
    "    'is_unemployed':10,\n",
    "    'job_search':7\n",
    "}\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print(column)\n",
    "    start = time.time()\n",
    "    trained_model = create_model(column, best_epochs[column])\n",
    "    print('load model:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "    start = time.time()\n",
    "    trained_model.validate()    \n",
    "#     print(val.shape)\n",
    "    # start_time = time.time()\n",
    "#     texts = val['text'].values.tolist()\n",
    "#     print(len(texts))\n",
    "#     predictions = trained_model.predict_batch(texts)\n",
    "    # predictions.head()\n",
    "    print('validation:', str(time.time() - start_time), 'seconds')   \n",
    "    \n",
    "    output = pickle.load( open( \"/scratch/da2734/twitter/8-boundary-classified/roc_auc_save_to_plot_binary.pkl\", \"rb\" ) )\n",
    "\n",
    "    ROC_dict[column] = output\n",
    "    \n",
    "#     break\n",
    "\n",
    "pickle.dump( ROC_dict, open( \"./ROC_dict_8.2.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ROC plot from learner.validate()\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# with open(\"./ROC_dict_8.2.pkl\", 'rb') as f:\n",
    "#     # The protocol version used is detected automatically, so we do not\n",
    "#     # have to specify it.\n",
    "#     ROC_dict = pickle.load(f)\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    \n",
    "    fpr = ROC_dict[column]['fpr']\n",
    "#     print(fpr)\n",
    "    tpr = ROC_dict[column]['tpr']\n",
    "    thresholds = ROC_dict[column]['thresholds']\n",
    "#     print(thresholds)\n",
    "#     print(fpr[\"micro\"])\n",
    "#     print(len(fpr[\"micro\"]), len(thresholds))\n",
    "    roc_auc = ROC_dict[column]['roc_auc']\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='{0} (area = {1:0.2f})'\n",
    "                                   ''.format(column, roc_auc[\"micro\"]))\n",
    "#     plt.annotate(thresholds, (fpr[\"micro\"], tpr[\"micro\"]))\n",
    "\n",
    "    count = 0\n",
    "    for t,x,y in zip(thresholds, fpr[\"micro\"],tpr[\"micro\"]):\n",
    "#         print(t,x,y)\n",
    "        count = count + 1\n",
    "\n",
    "        label = \"{:.3f}\".format(t)\n",
    "\n",
    "        if count%20 == 0:\n",
    "            plt.annotate(label, # this is the text\n",
    "                     (x,y), # this is the point to label\n",
    "                     textcoords=\"offset points\", # how to position the text\n",
    "                     xytext=(0,10), # distance from text to points (x,y)\n",
    "                     ha='center'\n",
    "                    ) # horizontal alignment can be left, right or center\n",
    "    plt.annotate(label, # this is the text\n",
    "         (x,y), # this is the point to label\n",
    "         textcoords=\"offset points\", # how to position the text\n",
    "         xytext=(0,10), # distance from text to points (x,y)\n",
    "         ha='center'\n",
    "        ) # horizontal alignment can be left, right or center\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--') #line y=x\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = all_pred['is_unemployed']\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ROC plot using predicted values on validation set directly i.e. not through validation\n",
    "\n",
    "import pickle\n",
    "\n",
    "# with open(\"./all_pred_8.2.pkl\", 'rb') as f:\n",
    "#     # The protocol version used is detected automatically, so we do not\n",
    "#     # have to specify it.\n",
    "#     all_pred = pickle.load(f)\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print(column)\n",
    "    merged = all_pred[column]\n",
    "    print(merged.head())\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    y_true = merged['pos']\n",
    "    y_pred = merged['pos_model']\n",
    "\n",
    "#    print(y_true)\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    " \n",
    "    fpr[\"micro\"], tpr[\"micro\"], thresholds = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "#     print('fpr[\"micro\"]', fpr[\"micro\"])\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    print(fpr[\"micro\"])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='{0} (area = {1:0.2f})'\n",
    "                                   ''.format(column, roc_auc[\"micro\"]))\n",
    "    count = 0\n",
    "    for t,x,y in zip(thresholds, fpr[\"micro\"],tpr[\"micro\"]):\n",
    "#         print(t,x,y)\n",
    "        count = count + 1\n",
    "\n",
    "        label = \"{:.3f}\".format(t)\n",
    "\n",
    "        if count%20 == 0:\n",
    "            plt.annotate(label, # this is the text\n",
    "                     (x,y), # this is the point to label\n",
    "                     textcoords=\"offset points\", # how to position the text\n",
    "                     xytext=(0,10), # distance from text to points (x,y)\n",
    "                     ha='center'\n",
    "                    ) # horizontal alignment can be left, right or center\n",
    "    plt.annotate(label, # this is the text\n",
    "         (x,y), # this is the point to label\n",
    "         textcoords=\"offset points\", # how to position the text\n",
    "         xytext=(0,10), # distance from text to points (x,y)\n",
    "         ha='center'\n",
    "        ) # horizontal alignment can be left, right or center\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--') #line y=x\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "    break\n",
    "    \n",
    "# merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "thresholds_dict = {\n",
    "    'is_unemployed': 0.16,\n",
    "    'lost_job_1mo':  0.03,\n",
    "    'job_search':  0.067,  \n",
    "    'is_hired_1mo':  0.02,\n",
    "    'job_offer':     0.111,\n",
    "}\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "# for column in [\"job_offer\"]:\n",
    "    merged = all_pred[column]\n",
    "    merged['neg'] = abs(1-merged['pos'])\n",
    "#     merged\n",
    "    threshold = thresholds_dict[column]\n",
    "#     print(threshold)\n",
    "    \n",
    "#     print(merged.loc[merged['pos_model'] > 0.9])\n",
    "    TP = np.sum(merged.loc[merged['pos_model'] > threshold, 'neg'])\n",
    "    FP = np.sum(merged.loc[merged['pos_model'] > threshold, 'pos'])\n",
    "    TN = np.sum(merged.loc[merged['pos_model'] < threshold, 'neg'])\n",
    "    FN = np.sum(merged.loc[merged['pos_model'] < threshold, 'pos'])\n",
    "#     TN = np.sum(merged.loc[merged['neg_model'] > threshold, 'neg'])\n",
    "#     FN = np.sum(merged.loc[merged['neg_model'] > threshold, 'neg'])\n",
    "    \n",
    "    total = sum([TP, FP, TN, FN]) \n",
    "#     print(threshold, TP/total, FP/total, TN/total, FN/total, total)\n",
    "    print(column, threshold, 'TPR:', TP/(TP+FN), 'FPR:', FP/(FP+TN), total)\n",
    "\n",
    "    confusion_matrix = np.array([\n",
    "                                [TN/total, FN/total],\n",
    "                                [FP/total, TP/total]\n",
    "                                ])\n",
    "    labels =           np.array([\n",
    "                                ['TN', 'FN'],\n",
    "                                ['FP', 'TP']\n",
    "                                ])\n",
    "    \n",
    "    labels = (np.asarray([\"{0} {1:.3f}\".format(string, value)\n",
    "                      for string, value in zip(labels.flatten(),\n",
    "                                               confusion_matrix.flatten())])\n",
    "         ).reshape(2, 2)\n",
    "#     print(labels)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(confusion_matrix, annot=labels, fmt=\"\", cbar=False)\n",
    "    ax.set(xlabel='Actual', ylabel='Predicted')\n",
    "    plt.title(column)\n",
    "    \n",
    "#     break\n",
    "\n",
    "# load from pickles\n",
    "# why am I getting different results between ROC curve and confusion matrices at the same thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    model_output = all_pred[column]\n",
    "    model_output['flip_pos_model'] = model_output['pos_model']\n",
    "    model_output['flip_neg_model'] = model_output['neg_model']\n",
    "\n",
    "    neg_sorted = model_output.sort_values(by=['flip_neg_model'], ascending = False)\n",
    "    pos_sorted = model_output.sort_values(by=['flip_pos_model'], ascending = False)\n",
    "\n",
    "    print('\\n\\n\\n------ ', column)\n",
    "    print('<<<< top 10 positive >>>> max prob:', max(model_output['flip_pos_model']))\n",
    "    print(pos_sorted['text'][:5])\n",
    "    print('\\n<<<< top 10 neg >>>> max prob:', max(model_output['flip_neg_model']))\n",
    "    print(neg_sorted['text'][:5])\n",
    "\n",
    "#     break\n",
    "    \n",
    "# pos_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
