{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2734/miniconda3/envs/worldbank/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "#gets all this setup\n",
    "import time\n",
    "start_time = time.time()\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "# from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc, accuracy\n",
    "from fast_bert.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "\n",
    "\n",
    "def create_model(columnm, epoch):\n",
    "    \n",
    "    if not os.path.exists('/scratch/da2734/twitter/mturk_mar6/model_log_100M_{}/'.format(column)):\n",
    "        os.makedirs('/scratch/da2734/twitter/mturk_mar6/model_log_100M_{}/'.format(column))\n",
    "\n",
    "#     if not os.path.exists('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column)):\n",
    "#         os.makedirs('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column))\n",
    "\n",
    "    LOG_PATH = Path('/scratch/da2734/twitter/batch/model_log_100M_{}/'.format(column))\n",
    "    DATA_PATH = Path('/scratch/da2734/twitter/mturk_mar6/data_binary_class_balanced_UNDERsampled/')\n",
    "    LABEL_PATH = Path('/scratch/da2734/twitter/mturk_mar6/data_binary_class_balanced_UNDERsampled/')\n",
    "    OUTPUT_PATH = Path('/scratch/da2734/twitter/mturk_mar6/output_binary_pos_neg_balanced_{}'.format(column))\n",
    "    FINETUNED_PATH = None\n",
    "\n",
    "    args = Box({\n",
    "        \"run_text\": \"labor mturk ar 6 binary\",\n",
    "        \"train_size\": -1,\n",
    "        \"val_size\": -1,\n",
    "        \"log_path\": LOG_PATH,\n",
    "        \"full_data_dir\": DATA_PATH,\n",
    "        \"data_dir\": DATA_PATH,\n",
    "        \"task_name\": \"labor_market_classification\",\n",
    "        \"no_cuda\": False,\n",
    "        #     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "        \"output_dir\": OUTPUT_PATH,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"do_lower_case\": True,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_train_epochs\": 100,\n",
    "        \"warmup_proportion\": 0.0,\n",
    "        \"no_cuda\": False,\n",
    "        \"local_rank\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"optimize_on_cpu\": False,\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"max_steps\": -1,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"logging_steps\": 50,\n",
    "        \"eval_all_checkpoints\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"overwrite_cache\": True,\n",
    "        \"seed\": 42,\n",
    "        \"loss_scale\": 128,\n",
    "        \"task_name\": 'intent',\n",
    "        \"model_name\": 'bert-base-uncased',\n",
    "        \"model_type\": 'bert'\n",
    "    })\n",
    "\n",
    "    import logging\n",
    "\n",
    "    logfile = str(LOG_PATH / 'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "        handlers=[\n",
    "            logging.FileHandler(logfile),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ])\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    logger.info(args)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        args.multi_gpu = True\n",
    "    else:\n",
    "        args.multi_gpu = False\n",
    "\n",
    "    label_cols = ['class']\n",
    "\n",
    "    databunch = BertDataBunch(\n",
    "        args['data_dir'],\n",
    "        LABEL_PATH,\n",
    "        args.model_name,\n",
    "        train_file='train_{}.csv'.format(column),\n",
    "        val_file='val_{}.csv'.format(column),\n",
    "        label_file='label_{}.csv'.format(column),\n",
    "        # test_data='test.csv',\n",
    "        text_col=\"text\",  # this is the name of the column in the train file that containts the tweet text\n",
    "        label_col=label_cols,\n",
    "        batch_size_per_gpu=args['train_batch_size'],\n",
    "        max_seq_length=args['max_seq_length'],\n",
    "        multi_gpu=args.multi_gpu,\n",
    "        multi_label=False,\n",
    "        model_type=args.model_type)\n",
    "\n",
    "    num_labels = len(databunch.labels)\n",
    "    print('num_labels', num_labels)\n",
    "\n",
    "    print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "    # metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "    metrics = []\n",
    "    # metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "    # metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "    # metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "    metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "    metrics.append({'name': 'roc_auc_save_to_plot_binary', 'function': roc_auc_save_to_plot_binary})\n",
    "    # metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n",
    "\n",
    "    learner = BertLearner.from_pretrained_model(\n",
    "        databunch,\n",
    "        pretrained_path='../mturk_mar6/output_binary_class_balanced_OVERsampled{}/model_out_{}/'.format(column, epoch), \n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "        logger=logger,\n",
    "        output_dir=args.output_dir,\n",
    "        finetuned_wgts_path=FINETUNED_PATH,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        multi_gpu=args.multi_gpu,\n",
    "        is_fp16=args.fp16,\n",
    "        multi_label=False,\n",
    "        logging_steps=0)\n",
    "    \n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def get_env_var(varname,default):\n",
    "    \n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname,':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname,':', var,'(Default)')\n",
    "    return var\n",
    "\n",
    "# Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "SLURM_JOB_ID            = get_env_var('SLURM_JOB_ID',0)\n",
    "SLURM_ARRAY_TASK_ID     = get_env_var('SLURM_ARRAY_TASK_ID',0)\n",
    "SLURM_ARRAY_TASK_COUNT  = get_env_var('SLURM_ARRAY_TASK_COUNT',1)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "path_to_data='/scratch/spf248/twitter/data/classification/US/'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Load Filtered Tweets:')\n",
    "# filtered contains 8G of data!!\n",
    "start_time = time.time()\n",
    "\n",
    "paths_to_filtered=list(np.array_split(\n",
    "glob(os.path.join(path_to_data,'filtered','*.parquet')),SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID])\n",
    "print('#files:', len(paths_to_filtered))\n",
    "\n",
    "tweets_filtered=pd.DataFrame()\n",
    "for file in paths_to_filtered:\n",
    "    tweets_filtered=pd.concat([tweets_filtered,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "\n",
    "print('time taken to load keyword filtered sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_filtered.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Load Random Tweets:')\n",
    "# random contains 7.3G of data!!\n",
    "start_time = time.time()\n",
    "\n",
    "paths_to_random=list(np.array_split(\n",
    "glob(os.path.join(path_to_data,'random','*.parquet')),SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID])\n",
    "print('#files:', len(paths_to_random))\n",
    "\n",
    "tweets_random=pd.DataFrame()\n",
    "for file in paths_to_random:\n",
    "    tweets_random=pd.concat([tweets_random,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Predictions of Filtered Tweets:')\n",
    "start_time = time.time()\n",
    "predictions_filtered = learner.predict_batch(tweets_filtered['text'].values.tolist())\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Predictions of Random Tweets:')\n",
    "start_time = time.time()\n",
    "predictions_random = learner.predict_batch(tweets_random['text'].values.tolist())\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Save Predictions of Filtered Tweets:')\n",
    "start_time = time.time()\n",
    "\n",
    "df_filtered = pd.DataFrame(\n",
    "[dict(prediction) for prediction in predictions_filtered],\n",
    "index=tweets_filtered.tweet_id).rename(columns={\n",
    "'is_unemployed':'unemployed',\n",
    "'job_search':'search',\n",
    "'is_hired_1mo':'hired',\n",
    "'lost_job_1mo':'loss',\n",
    "'job_offer\"':'offer',\n",
    "})\n",
    "\n",
    "df_filtered.to_csv(\n",
    "os.path.join(root_path,'pred','filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'))\n",
    "\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Save Predictions of Random Tweets:')\n",
    "start_time = time.time()\n",
    "\n",
    "df_random = pd.DataFrame(\n",
    "[dict(prediction) for prediction in predictions_random],\n",
    "index=tweets_random.tweet_id).rename(columns={\n",
    "'is_unemployed':'unemployed',\n",
    "'job_search':'search',\n",
    "'is_hired_1mo':'hired',\n",
    "'lost_job_1mo':'loss',\n",
    "'job_offer\"':'offer',\n",
    "})\n",
    "\n",
    "df_random.to_csv(\n",
    "os.path.join(root_path,'pred','random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'))\n",
    "\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
